\documentclass[hidelinks, 12pt, oneside]{article}
\usepackage{bookmark}
\usepackage{graphicx}
\usepackage{hyperref}
\graphicspath{{images/}}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\begin{document}
 
%titlepage
\thispagestyle{empty}
\begin{center}
\begin{minipage}{0.75\linewidth}
    \centering
    
%University logo
    \includegraphics{UPlogo}
    \rule{0\linewidth}{0.15\linewidth}\par

%Thesis title
    {\uppercase{\Large COS 301 Mini Project\par}}
   	{\Large Top Level Integration Testing \par} 
    \vspace{1cm}
%Author's name
    {\normalsize Elana Kuun u12029522\par}
    {\normalsize Hlavutelo Maluleke u12318109\par}
    {\normalsize Estian Rosslee u12223426\par}
    {\normalsize David Breetzke u12056503\par}
    {\normalsize Sylvester Mpungane u11241617\par}
    {\normalsize Phethile Mkhabela u12097561\par}
    {\normalsize Renaldo van Dyk  u12204359\par}
    {\normalsize Antonia Michael  u13014171\par}
    {\normalsize Herman Keuris  u13037618\par}
    {\normalsize Jaco-Louis Kruger  u13025105\par}
    \vspace{1cm}
    
    \href{https://github.com/Jaco-Louis/Top-level-testing.git}{Github Repository link}\par
    \vspace{1cm}
%Date
    {\Large April 2015}
\end{minipage}
\end{center}
\clearpage

\tableofcontents

\newpage

\section{Introduction}
The testing phase of the mini project requires thorough testing of all the aspects of both systems developed. This includes both functional and non-functional aspect of the systems. The Buzz system aimed to assist users to collaborate and share knowledge with fellow students. The system was designed in order to create modules for the various functionality required, these modules needed to be integrated to create the desired system. Both systems were developed and needed to be tested. The testing of both systems will be discussed in the following sections.

\section{Testing Results}
\subsection{Top Level A Testing Results}
\subsubsection{Functional Testing}
\begin {enumerate}
\item Buzz-Spaces Module

\input {buzzspacesA}

\item Buzz-Data-Sources
\input {dataSources}

\item Buzz Status
\input {status}

\item Buzz-reporting
\input {reporting}

\end {enumerate}
 
 

\subsubsection{Non-functional Testing} 

\begin{enumerate}
\item Maintainability: 

In order for a system to be maintainable, the system should be both flexible and extensible.  For developers to continue maintenance on the system, it would require that the system should be easy to understand, the technologies used should be available for an extended period of time and developers should be able to easily add new functionality to the system. 

The code tested satisfies the condition that it should be able to understand the system. The system developed is not cluttered and difficult to understand due to good extraction between layers in the Model View Controller model. An obstacle in trying to understand the system is that the code has not been thoroughly documented. This causes some difficulties when trying to gain a better context about certain functions, why they are there and what their purposes are.

The technologies chosen satisfies the requirement of having to be available for a long period of time. The technologies used (such as Node js), are relevantly new platform and therefore they are constantly being updated. This will ensure long use of the technologies, enhancing maintainability.

Developers would be able to easily maintain the code in the sense of adding new features. This is made possible for example by making use of the simple way to add a new route to the program. There might however be slight difficulties when trying to change certain aspects of the system, due to the system being developed too specific, rather than focusing on adaptability. More emphasis was put on being able to easily add to the system, rather than changing the system, though it will still be possible it would only take longer than adding to the system.

\item Scalability:

In order for a system to be scalable, the system should be able to expand and scale in order to handle a larger load or to serve more clients. 

The system satisfies scalability in the sense that it is currently able to handle all the modules for the computer science department. This is however only a small portion of what the system should be capable of doing. The system should theoretically be able to scale in order to service 50 000 students. This should not be a problem for the system, due to the technologies that it are using. Node js is a lot more scalable than Apache, even though it is only single threaded by default. The use of MongoDB, rather than MySql also increases the scalability of the server [1].  The scalability of Node js can be increased by using multi-core CPUs rather than a single core CPU and load balancers can be implemented. This will ensure that the server application scales more than enough in order to be able to serve the required amount of users. This can however only be done when purchasing more efficient hardware for the host machine. 

\item Performance:

The performance requirements set out in the formal specification stated that all non reporting operations should respond within less than 0.2 seconds. The specification also stated that report queries should not take any longer than 5 seconds.

The performance testing was done using Firebug for Firefox. Almost all of the non reporting requests responded well under 0.1 seconds. The only cases where the server did not respond fast enough, was when the page was initially requested. The initial page request resulted in a response time of 1.33s which was the worst case. On average the initial request for the homepage resulted in about 305ms response time. 
The next case that took longer than it should, is when a page is requested that does not exist. The reason for this decrease in performance is due to the server searching for a route that does not exist. The server will then eventually just give up and return the error page. This resulted in an average response time of about 286ms. 

The overall average response time for requests is about 150ms as determined by firebug. Below are some screenshots created during the testing of non reporting requests.

The performance for the reporting use cases exceeded expectations by performing well under the recommended 5s. This might however change as the database size increases. Testing was limited to the usage of a fairly small size database. 

On average the generating of a report averaged at about 310ms. Below are some screenshots that was taken during the performance testing of the report requests. 
 
\item Security 

The system is not secure in all areas. It requires a username and password in order for a user to login. The system successfully performs authentication against the LDAP repository. It does not allow users without the necessary authorisation to delete buzz spaces, but anyone can create a buzz space whether they are logged in or not. Only authorised users can add or remove admins. The system is also somewhat configurable, MIME types can be added, edited, and removed, but the system does not take authorisation level or whether a user is logged in into account for these actions, anyone can perform these actions. Admin users cannot configure access to certain services.

The log in user interface:

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{logFile} 
\end{figure}

Determine if the user is authorised to close the buzz space:


\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{closeBuzzSpace} 
\end{figure}

Assign an admin to a BuzzSpace:


\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{addAdmin} 
\end{figure}


\item Auditability:

The system is auditable. 
The system keeps a log of all the requests and responses. The requests does not have an id, but it does have a user id, a date/time stamp, it includes the request that was made, and it does not store any sensitive information.

For the responses, there is not an id for the log entry or a corresponding request entry, there is a date/time stamp, and there is no sensitive information included. Because the response entries does not have a way of indicating to which request it responded it can be difficult to make sense of the date or to trace errors.

The audit log can be accessed by humans and the system. 


\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{logFile} 
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{logs} 
\end{figure}

An example of responses that are logged:
A buzzSpace does not exist:

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{buzzNotExistLog} 
\end{figure}

Could not find space to remove:

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{couldNotFindSpaceLog} 
\end{figure}

Request to manage space COS9:

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{manageSpaceLog} 
\end{figure}

Response that states a space was closed successfully:

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{ClosedSpaceLog} 
\end{figure}

\item Testability:
Only some modules included unit tests. For example, threads and spaces:
Unit testing for threads:

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{unit_tests} 
\end{figure}


Integration tests are also performed in some instances. 

In this instance authorisation is checked outside of the module that is was created in.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{integration} 
\end{figure}


For both the user tests and the integration tests it is verified that all the pre-conditions are met 

The system is thus only testable in some cases, but there is a lot of potential to add further test cases as the system is set up in such a way that makes testing more convenient.
 
\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{performance1}
    \caption{Non reporting based request for constraints}
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{performance2}
    \caption{Non reporting based request for contributors}
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{performance3}
    \caption{Non reporting based request for test Post. Results in 404 error}
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{performanceR1}
    \caption{Reporting based request}
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{performanceR2}
    \caption{Reporting based request}
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{performanceR3}
    \caption{Reporting based request}
\end{figure} 
\item Usability:

The system is usable because firstly the necessary actions that a user can take appear in a navigation bar at the top of the screen. Thus it is easy for a novice user to be able to know what to click on and navigate the website. 

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{UsabilityScreenshotNavBar.png} 
\end{figure}

The interface is not cluttered, and only basic functionality is displayed on the home screen, making the system more learnable. The buttons are labelled with text rather than with graphical icons, and the text on the button is quite explanatory, which makes their purpose more clear. 

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{UsabilityExplanatoryButtons} 
\end{figure}

Larger headings are used to label the different sections, for example under the Manage Constraints tab there are large headings to indicate the Existing constraints section and the Add new constraint section. This again contributes to ease of use for the novice first year user.

Through these mechanisms the system is memborable hence it is also be understandable.  

\item Integratibility:

The system is able to address future integration requirements by providing access to its services using widely adopted public standards such as firstly having separate npm packages for all the different modules. The packages are stored on Synopia. Also, electrolyte is being used in the server to provide a dependency injection. The HandleBars server is the main server that needs to be used to test and integrate all the modules on. The routes/index.js, routes/infrastructure.js and routes/content.js files use express to route the different hbs files for the different modules in order to integrate the infrastructure and content subsystems into the main system. 

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{IndexContentInfra} 
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{HbsFiles} 
\end{figure}

A separate file is used to establish the connection to the database to avoid having this done in all the separate files. Also, global variables are now used such as the global password and username for example. 

A document has been provided via email and a README file has been provided to specify important standards and regulations that must be followed.

The functional code in the separate packages must be placed in an exportable function taking parameters such as the database or settings, and this is done as part of the electrolyte dependency injection. 

Exports are also used in the different files to make the code accessible to the other files.

The following screenshot indicates the systems' integratibility because it is clear that the following function is able to accept standards such as json strings for objects. This is intergrateble because the interface does not know which functions are sending the aforementioned objects. Therefore it allows the interface to be connected to any backend system.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{JsonObject} 
\end{figure}


\item Deployability:

The system is deployable on Linux servers as we have run it using Ubuntu 14.04 Linux and the system was able to run. The following screenshot shows the system running on a Linux server:

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{LinuxServerScreenShot} 
\end{figure}

The system is deployable on an environment using different databases for persistence of the Buzz database because the Handlebars server contains a folder called node\_modules, and it contains the buzz\_database package. This package can easily be swapped out and an alternative database package can be plugged in, that the system can use due to the flexibility of this server. As long as the new package has the same name so that the files that require the database do not need to be changed, no major changes will need to be made.  
 

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{DatabaseSchema} 
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{databaseCurrent} 
\end{figure}

The system is deployable in environments where the user authentication credentials and roles are sourced from different repositories. The following screenshot indicates that the Handlebars server contains a folder called node\_modules, and it contains the buzz\_csds package. 


\begin{figure}[h!]
  \centering
    \includegraphics[width=0.85\textwidth]{CSDSPackage} 
\end{figure}
 
This package can easily be swapped out and an alternative data source package with different credentials and roles can be used, due to the pluggability of the system and dependency injection employed by the top level team. The new package must just be named the same as the old one to avoid errors where the file is required from the package in the code. 

\end{enumerate}


\subsection{Top Level B Testing Results}

\subsubsection{Functional Testing}

\begin{enumerate}
\input{buzzResourcesBModule}
\input{buzzBNotification}
\input{statusB}
\input{BuzzDataSourcesModule}
\end{enumerate}

\subsubsection{Non-functional Testing} 

\begin{enumerate}
\input{maintainabilityB}
\input{scalabilityB}
\input{auditability}
\input{testability}
\input{performance}
\input{reliabilityavailibility}
\end{enumerate}

\newpage
\section{Conclusion}

The Buzz systems had been tested, evaluated and compared to the original specification that defined how the system should be implemented. The evaluation of both implementations have brought up several interesting facets. It has been noted that both systems function to an extent, but none of the two implementations fulfilled the requirements completely. The systems implemented showed some promise in terms of some modules when viewed in isolation, while some modules did not function completely. Some modules failed to satisfy testing requirements such as unit tests, while others excelled at their unit tests. The integration of several related modules led to great success while others failed. At the top level some implementations delivered a well-functioning system, but lacked in terms of security and completeness. The other implementation led to a well functioning interfaced system, but lacked in terms of functionality. The testing of both systems led to the conclusion that the systems both worked to some extent, but failed to satisfy the complete requirements of the original specification of the system.

\end{document}
